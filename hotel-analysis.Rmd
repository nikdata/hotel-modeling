---
title: "Hotel"
author: "Nik Agarwal"
date: "4/14/2020"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

One of my main motivations is to learn how to develop models using a GPU. Model development using a GPU is quite a bit different than simply building a model in-memory. For starters, you need a GPU! What's interesting is that you need a *compatible* GPU that works with an appropriate framework. For instance, if you want to use TensorFlow, you have to have a compatible nVidia GPU. However, what if you use newer Macs (post 2015) that are only compatible with AMD GPUs? That's where another framework called [PlaidML](https://github.com/plaidml/plaidml) comes to the rescue.

PlaidML is an alternative framework to TensorFlow that enables you to use almost any compatible GPU - such as AMD Radeon, nVidia, etc. You can find out how to set-up PlaidML [here](https://plaidml.github.io/plaidml/docs/install#macos). While PlaidML may not be as mature as TensorFlow, you can easily use Keras (the high-level wrapper) and continue on your way. Sure, I could have used Google CoLab or AWS to learn how to build models using a GPU (and stuck with TensorFlow). However, I prefer to do my analysis/coding on a local machine in a non-notebook environment. But I digress.

Surprisingly, I found it challenging to find simple yet comprhensive examples for regression and classification. Many of the examples I found were around image classification, text analytics, etc. Even the 'simple' regression example (see [here](https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/)) uses a dataset that is pretty much ready for a neural network. I am not 'knocking' on the author, but I have to admit that if you're learning how to build models on a GPU, sometimes it is worthwhile to use a dataset that isn't 'analytics-ready' so you can see the steps involved to get to the actual modeling phase. Or maybe that's how I learn? Either way, I figured it's best if I create my own example.

I have three main goals for this exercise: 

- Pick a dataset and walk through a 'usual' data-science process (e.g., exploratory data analysis - EDA, feature engineering, train/test split, etc.)
- Develop a simple neural network model using Keras
- Apply trained model to test dataset

**NOTE**: I am not interested in making the 'perfect' model or even a well-performing model. I am simply creating a walkthrough on how to go from a dataset to trained neural network (i.e., deep neural network - DNN) model using a GPU, PlaidML, and Keras.


# Background on Dataset

On February 11, 2020, the [TidyTuesday](https://github.com/rfordatascience/tidytuesday) project published an interesting dataset around [hotel bookings](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11). This dataset is actually from a [research paper](https://www.sciencedirect.com/science/article/pii/S2352340918315191#f0010) that was published in 2019.

Briefly, the dataset captures the bookings made for customers arriving between July 1, 2015 and August 31, 2017 between two different hotel types (resort vs. city).

While this is is a great exploratory dataset, I'm choosing to predict whether a booking will be cancelled or not (a classic two-class problem). Contextually speaking, this dataset may not perform well because it may not capture the true reasons why a booking was cancelled. However, it is a large enough dataset that I can use as a walkthrough.

I am aware that this dataset is fairly 'clean' and ready to be analyzed. And that's ok! This dataset is actually not perfect:

- it has missing values
- it has several categoricals
- new features can be created

In my view, this dataset is perfect!

# Load Data

I will be making use of several libraries for this walkthrough:

- {vroom}: for reading in the dataset
- {tidyverse}: for data wrangling and plots
- {rsample}: for creating training/test dataset
- {recipes}: for processing both training & test datasets
- {keras}:  for neural network using GPU

```{r load_libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(rsample)
library(recipes)
library(keras)
```

Now let's load the dataset:

```{r load_data}
hotels <- vroom::vroom(
  file = "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv",
  na = c("", " ", "NA", "NULL", 'Undefined'))
```

Have you used vroom? It's basically a 'competitor' to the {data.table} package, but aligned with the tidyverse grammar. {data.table} is a great package and is FAST at almost everything. However, I prefer vroom over {data.table}'s `fread()` function.

One of the interesting things about `vroom()` is that you can identify what makes up an *na* or NULL value. In this case, I populated some example terms that I have seen used to identify NULL values. In my experience, this is often the case where NULL values are actually identifed as "NULL" or "NA" (in other words, these are characters).

Now let's take a quick peek at the data using the `glimpse()` function (part of {dplyr}).

```{r df_preview}
glimpse(hotels)
```

# Check for Missing Values

One of the first things I like to do is check for missing values. This is great for identifying variables (i.e., predictors) that we can exclude early on because they have too many missing values or variables which require imputation.

```{r null_chk, message=FALSE, warning=FALSE}
hotels %>%
  purrr::map_df(function(x) sum(is.na(x))) %>%
  tidyr::pivot_longer(
    cols = everything(),
    names_to = "variable",
    values_to = 'missing'
  ) %>%
  dplyr::arrange(desc(missing)) %>%
  dplyr::mutate(
    percent_missing = round(missing / nrow(hotels),2)
  ) %>%
  dplyr::filter(missing > 0)
```

It seems like there are seven (7) different variables that have missing values. Note how almost 94% of the values in the variable <company> are missing. I can confidently say that this variable is not worth imputing or keeping in our analysis.

## Conclusions
- Remove the variable <company> from the entire training.
- We can consider removing the variable <agent> since 14% of the observations are missing
- We can consider simply removing the missing observations (i.e., rows) for country, distribution channel, market segment, and children
- We can consider imputing the missing values for meal

# Variable Exploration

After checking for null values, I like to explore a few interesting variables further. For the purposes of this walkthrough, I'm going to focus on the following variables:

- agent
- hotel type
- cancellation (the response variable <is_canceled>)

Generally, this would be more exhaustive, but for this walkthrough it is not as important.

## agent

Let's explore the variable <agent> further. First off, let's see how many unique values exist:

```{r agent_unique_values}

length(unique(hotels$agent))

```

There are 334 unique values for the variable <agent>. This includes the NULL value. Typically, we can use the {forcats} package to trim down the number of unique values. For instance, we can identify the top 5 values and then have a 6th value that simply means "all others". However, if you recall, our goal is to predict cancellation. This column seems to be a unique identifier for the travel agent or tour operator. However, I'm not sure if this column will be contextually important. Don't forget, it also is missing 14% of the values!

For the purposes of this walkthrough, I'll simply remove it.


## is_canceled (the response variable)

The response variable <is_canceled> is essentially a binary column: 1 meaning true, 0 meaning false. Let's see if these outcomes are balanced:

```{r response_variable_balance_chk}
hotels %>%
  dplyr::group_by(is_canceled) %>%
  dplyr::count() %>%
  dplyr::rename('observation_count' = 'n') %>%
  dplyr::mutate(
    pct = round(observation_count / nrow(hotels),2)
  )
```

The table shows that this is about a 60/40 split. It's not really well-balanced. This is good to know when we go to split our dataset into training and test datasets. We'll want to make sure that we apply appropriate strategies to avoid an unbalanced dataset.

## hotel

This variable identifies what type of hotel the observation is for. For instance, is it a city hotel or a resort hotel?

```{r cityvsresort}
hotels %>%
  group_by(hotel) %>%
  count() %>%
  rename(observation_count = n) %>%
  mutate(
    pct = round(observation_count / nrow(hotels),2)
  )
```

It seems that almost 70% of the booking data in our dataset are for the city hotel. This makes sense intuitively as the city hotel will probably have far more business-related bookings (i.e., more bookings of shorter durations) than a resort hotel which is geared more towards vacationers. Also, the locations of the hotels and amenities may vary.


# Feature Engineering

After having spent some time with this dataset, there are definitely a few features that I want to create (before I dive further into some EDA):

- Remove the agent variable
- discard rows that have missing values for country & children
- create feature that looks at the day of week of arrival (M, T, W, etc.)
- create feature that tells the duration of stay (planned or actual)
- create feature that identifying total guests reservation is for
- total cost of stay
- create feature if meal was included in reservation or not (per the data dictionary, all NULL values mean no meal was included)
- convert the variable hotel to binary (1 for city hotel, 0 for resort)

One of first things that I like to do before creating or deleting columns and rows is to create another dataframe.

```{r create_newdf}

cln_hotel <- hotels

```

## Feature:: Drop Missing Observations

Recall from earlier that the variables <children> and <country> had missing observations. However, since there weren't many observations missing, it was easier to simply drop the rows that had missing values for these two respective columns.

```{r feat_dropobservations}
cln_hotel <- cln_hotel %>%
  tidyr::drop_na(children, country)
```


## Feature: day of week

Here we will combine the separated date parts for arrival and determine the day of week (e.g., Monday, Tuesday, etc.)

```{r feat_dayofweek}

cln_hotel <- cln_hotel %>%
  dplyr::mutate(
    reservation_date = lubridate::ymd(paste0(arrival_date_year,"-",arrival_date_month,"-",arrival_date_day_of_month)),
    day_of_week = lubridate::wday(reservation_date, label = T)
  )

```

## Feature: Duration of Stay

This feature calculates the total number of nights a guest has booked for regardless of day of week.

```{r feat_duration}

cln_hotel <- cln_hotel %>%
  dplyr::mutate(
    stay_duration = stays_in_week_nights + stays_in_weekend_nights
  )
```


## Feature: Total Guests

This feature tallies up the total number of expected guests on a booking.

```{r feat_totalguests}

cln_hotel <- cln_hotel %>%
  dplyr::mutate(
    total_guests = adults + children + babies
  )

```


## Feature: Total Cost of Stay

Since we don't know the actual cost per night, we are able to use the variable <adr> which is the average daily rate and muliply that with the total number of nights booked (this variable was created above). We also filter the dataset to only include total costs that are more than 0. This ensures that we do not model very special circumstances.

```{r feat_costofstay}
cln_hotel <- cln_hotel %>%
  dplyr::mutate(
    total_cost = stay_duration * adr
  ) %>%
  filter(total_cost > 0)
```


## Feature: Meal Included?

Since we know most stays include some type of meal, it is far easier to create a "boolean" column that describes if a booking has a meal included or not regardless of what kind of meal is included (if any).

```{r feat_meal}

cln_hotel <- cln_hotel %>%
  dplyr::mutate(
    meal_included = case_when(meal %in% c('SC','Undedfined') ~ 0, TRUE ~ 1)
  )

```


## Feature: City Hotel?

Finally, we know that there are two types of hotels: city & resort. It's simpler to create a feature that describes if the observation is for a city hotel or not. If not, we can safely assume it is for a resort hotel.

```{r feat_cityhotel}
cln_hotel <- cln_hotel %>%
  dplyr::mutate(
    city_hotel = ifelse(hotel == 'City Hotel', 1, 0)
  )
```



## Feature: Remove Variables

Since we created some new features, I'm going to remove some of the unneeded columns. Maybe these columns could improve the model, but for this tutorial's sake, we're just removing them.

```{r feat_removevars}

cln_hotel <- cln_hotel %>%
  select(
    -agent,
    -company,
    -arrival_date_year, 
    -arrival_date_month, 
    -arrival_date_week_number, 
    -arrival_date_day_of_month, 
    -stays_in_weekend_nights, 
    -stays_in_week_nights, 
    -adults, 
    -children, 
    -babies, 
    -reservation_status, 
    -reservation_status_date, 
    -adr, 
    -meal, 
    -distribution_channel, 
    -reservation_date
  )

```


# Train/Test Split

Typically, I create 3 different datasets: training, validation, and test. Essentially, the model is built on the training dataset and the validation dataset is used to 'prove' out the model. I only use the test dataset as the absolute final dataset in which to prove out my trained model. Generally speaking, if my model fails on the test dataset, but performs well on the training and validation datasets, I start to look at the way the data were split. However, many issues can contribute to a model performing well on both training and validation datasets, but not performing well on the test dataset.

For this case study, I am only going to create a training and test dataset as I'm not interested in building the perfect model (if one can call it that), but rather a model using Keras.

```{r splits}
set.seed(8837)
splits <- rsample::initial_split(cln_hotel, prop = 0.8, strata = is_canceled)

df_train <- splits %>% rsample::training()
df_test <- splits %>% rsample::testing()

```


# Prep Training


## Create Recipe
```{r recipe_def}

# define recipe definition

rec1 <- recipes::recipe(~., data = df_train) %>%
  recipes::update_role(is_canceled, new_role = 'outcome') %>%
  recipes::update_role(meal_included, city_hotel, is_repeated_guest, new_role = 'pred_binary') %>%
  recipes::step_normalize(recipes::all_numeric(), -recipes::has_role('pred_binary'), -is_canceled) %>%
  recipes::step_string2factor(recipes::has_type(match = 'nominal')) %>%
  recipes::step_ordinalscore(day_of_week) %>%
  recipes::step_integer(recipes::all_nominal()) %>%
  recipes::step_nzv(everything())
  
  

summary(rec1)

```



## Apply Recipe to Training DF
```{r}
rec1 <- recipes::prep(rec1, data = df_train, strings_as_factors = FALSE)
df_train <- recipes::juice(rec1, everything())
glimpse(df_train)

```


## Apply Recipe to Testing DF
```{r}
df_test <- recipes::bake(rec1, df_test)
glimpse(df_test)
```

# Model Development - GPU